{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica de Minería de Datos.\n",
    "\n",
    "#### Realizada por Araceli Macía Barrado\n",
    "\n",
    "##### Segunda Parte\n",
    "----------\n",
    "\n",
    "El objetivo de esta práctica es poder identificar a partir de la letra de la canciones el genero \"Reggueaton\".\n",
    "Dado que el texto de las canciones de este genero es muy identificativo, por el contenido sexual, referencias a alcohol o drogas o a armas de fuego; se deberia poder identificar por la letra cuando una cancion pertenece a este genero o no. \n",
    "<br>\n",
    "\n",
    "Dificultades encontradas: \n",
    "1. El lenguaje de las canciones Reggeaton, es una mezcla entre español-latino y Spanglish.\n",
    "2. Aunque parezca mentira, el Reggeaton evoluciona para mostrarse menos incorrecto.\n",
    "\n",
    "\n",
    "<br>\n",
    "La práctica la he dividido en los siguientes NoteBook:\n",
    "1. PT_MinDatos_Reg1 :  Código para la recuperacion de los textos de las canciones.\n",
    "2. PT_MinDatos_Reg2 : Código para tratamiento y clasificación de textos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "### Recuperación y Tratamiento de Datos.\n",
    "\n",
    "Para realizar la practica he descargado las letras de las canciones de la página : \n",
    "* http://www.sonicomusica.com\n",
    "\n",
    "Previo al ejercicio, he realizado consultas para seleccionar dos ficheros de canciones, una de canciones típicas de Reggeaton, y otra de canciones que no lo son. A partir de estos ficheros, que contienen los enlaces a las canciones, procederé a realizar la descarga de las canciones. Cada canción se descarga en un fichero, y el texto se guardará tokenizado. A su vez, los ficheros los he separado en dos directorios, para tener separadas las letras de las canciones de Reggeaton de las que no lo son.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importacion de librerias necesarias.\n",
    "import requests\n",
    "import lxml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from lxml import html\n",
    "import time\n",
    "from os import listdir\n",
    "import fnmatch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canciones_NR.txt  \u001b[34mNO_REG\u001b[m\u001b[m/           \u001b[34mREG\u001b[m\u001b[m/              canciones_RG.txt\r\n"
     ]
    }
   ],
   "source": [
    "#Reviso que estan los directorios.\n",
    "% ls FicDatos/    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words_file(ruta, fic):   #devolver las palabras de cada fichero.\n",
    "    ficLeido=open(ruta+fic, 'r')\n",
    "    try:\n",
    "        texto=ficLeido.read().decode('utf-8')\n",
    "        tokens =nltk.word_tokenize(texto)         #tokens=['a','b','c'] lo dejo como unicode.\n",
    "        ficLeido.close()\n",
    "        return tokens\n",
    "    except UnicodeDecodeError:     \n",
    "        print \"error en\", ruta+fic\n",
    "        return [\"---\"]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#funcion para leer los ficheros del directorio.\n",
    "def ficheros(ruta2,pa):\n",
    "    tot=[] \n",
    "    for fileid in listdir(ruta2):\n",
    "        if fnmatch.fnmatch(fileid, pa):\n",
    "             tot.append(fileid)\n",
    "    return tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def words_all_file(rutas):   #devolver las palabras de los ficheros. Devuelve una lista.\n",
    "    tokens=[]\n",
    "    \n",
    "    for t in rutas:\n",
    "        ruta1=t[0]\n",
    "        patron1=t[1]\n",
    "       \n",
    "        for fileid in ficheros(ruta1,patron1):\n",
    "            ficLeido1=open(ruta1+fileid, 'r')\n",
    "            try:\n",
    "                texto=ficLeido1.read().decode('utf-8')                \n",
    "                tokens.extend(nltk.word_tokenize(texto))        #tokens=['a','b','c'] UNICODE\n",
    "                ficLeido1.close()\n",
    "            except UnicodeDecodeError:     \n",
    "                print \"error en\", ruta+fic\n",
    "                return [\"---\"]\n",
    "    return tokens    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def document_features(document): \n",
    "# de cada documento, que contine la lista de palabras, devuelvo si contiene o no las palabras encontradas filtradas.\n",
    "        \n",
    "    document_words = set(document) \n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        w1=word.encode('utf-8')  #convierto a utf8 para poder formatearlo.\n",
    "        features['contains({})'.format(w1)] = (word in document_words)  \n",
    "    return features\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ruta1=\"FicDatos/REG/\"\n",
    "ruta2=\"FicDatos/NO_REG/\"\n",
    "\n",
    "#creo documento de tipos list:   (tokensCANCION,\"REG\")\n",
    "documents1 = [ (list(words_file(ruta1,fileid)), \"REG\")\n",
    "              for fileid in ficheros(ruta1,\"REG*.txt\")]\n",
    "\n",
    "#creo documento de tipos list:   (tokensCANCION,\"NO REG\")\n",
    "\n",
    "documents2 = [ (list(words_file(ruta2,fileid)), \"NO_REG\")\n",
    "              for fileid in ficheros(ruta2,\"NR*.txt\")]\n",
    "\n",
    "#Uno las listas, de forma que ahora tengo una super lista, donde cada elemento son las palabras de una cancion\n",
    "#y un indicador de si es Regueton o no lo es.\n",
    "documents= documents1 + documents2\n",
    "np.random.shuffle(documents)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numero de palabras total 104261\n"
     ]
    }
   ],
   "source": [
    "rutas=[(\"FicDatos/REG/\",\"REG*.txt\"),(\"FicDatos/NO_REG/\",\"NR*.txt\")]\n",
    "allwordsT=words_all_file(rutas) #recupero todas las palabras de los ficheros.\n",
    "print \"numero de palabras total\", len(allwordsT)  #Numero de palabras en total de todas las canciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de palabras total 104261\n",
      "numero de palabras sin repetir 9179\n"
     ]
    }
   ],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in allwordsT) #frecuencia de cada palabra, de todos los documentos.\n",
    "print \"Numero de palabras total\", len(allwordsT)  #Numero de palabras en total de todas las canciones.\n",
    "word_features = list(all_words)[:3000]  #Me quedo con las 3000 palabras de la lista\n",
    "print \"numero de palabras sin repetir\", len(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",;4476\n",
      "que;4046\n",
      "y;2677\n",
      "la;2535\n",
      "no;2176\n",
      "me;1998\n",
      "de;1983\n",
      "te;1920\n",
      "el;1852\n",
      "a;1783\n"
     ]
    }
   ],
   "source": [
    "for w, frequency in all_words.most_common(10):\n",
    "    print(u'{};{}'.format(w, frequency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280\n",
      "180\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print len(featuresets)\n",
    "print len (train_set)\n",
    "print len (test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))  #Tiene un acierto bastante alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteristicas mas Representativas: \n",
      "           contains(the) = True              REG : NO_REG =      9.7 : 1.0\n",
      "            contains(x2) = True              REG : NO_REG =      9.4 : 1.0\n",
      "         contains(llego) = True              REG : NO_REG =      7.7 : 1.0\n",
      "         contains(llama) = True              REG : NO_REG =      6.6 : 1.0\n",
      "           contains(fin) = True           NO_REG : REG    =      6.6 : 1.0\n",
      "             contains(:) = True           NO_REG : REG    =      6.6 : 1.0\n",
      "           contains(has) = True           NO_REG : REG    =      6.2 : 1.0\n",
      "             contains(2) = True              REG : NO_REG =      5.7 : 1.0\n",
      "            contains(ve) = True              REG : NO_REG =      5.7 : 1.0\n",
      "      contains(silencio) = True           NO_REG : REG    =      5.7 : 1.0\n",
      "       contains(sentido) = True           NO_REG : REG    =      5.0 : 1.0\n",
      "         contains(mucha) = True              REG : NO_REG =      5.0 : 1.0\n",
      "          contains(rico) = True              REG : NO_REG =      5.0 : 1.0\n",
      "       contains(aquella) = True              REG : NO_REG =      5.0 : 1.0\n",
      "      contains(bailando) = True              REG : NO_REG =      4.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cpdist = classifier._feature_probdist\n",
    "        \n",
    "print('Caracteristicas mas Representativas: ')\n",
    "\n",
    "for (fname, fval) in classifier.most_informative_features(15):\n",
    "    def labelprob(l):\n",
    "        return cpdist[l, fname].prob(fval)\n",
    "\n",
    "    labels = sorted([l for l in classifier._labels\n",
    "                     if fval in cpdist[l, fname].samples()],\n",
    "                    key=labelprob)\n",
    "    if len(labels) == 1:\n",
    "        continue\n",
    "    l0 = labels[0]\n",
    "    l1 = labels[-1]\n",
    "    if cpdist[l0, fname].prob(fval) == 0:\n",
    "        ratio = 'INF'\n",
    "    else:\n",
    "        ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n",
    "                           cpdist[l0, fname].prob(fval))\n",
    "    print(('%24s = %-14r %6s : %-6s = %s : 1.0' %\n",
    "           (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "\n",
    "Hasta aqui, no he realizado ningun tipo de filtrado en los datos de partida, ahora voy a filtrar para quitar caracteres no alfanumericos, y tambien voy a dejar de considerar las palabras que no esten en el diccionario de Español, a ver como reacciona el clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop=stopwords.words(\"spanish\")\n",
    "Spanish_vocab = set(w.lower() for w in nltk.corpus.cess_esp.words() )\n",
    "\n",
    "def words_all_file_SinCaraSTOPDIC(rutas):   #devolver las palabras de los ficheros. Devuelve una lista.\n",
    "    tokens=[]\n",
    "    \n",
    "    for t in rutas:\n",
    "        ruta1=t[0]\n",
    "        patron1=t[1]\n",
    "       \n",
    "        for fileid in ficheros(ruta1,patron1):\n",
    "            ficLeido1=open(ruta1+fileid, 'r')\n",
    "            try:\n",
    "                texto=ficLeido1.read().decode('utf-8')\n",
    "                t1=nltk.word_tokenize(texto)              \n",
    "                for w in t1:  \n",
    "                    wo=w.lower()\n",
    "                    if wo.isalpha():  #Solo considero palabras alfanumericas.\n",
    "                        if wo not in stop:  #solo considero las palabras que no estan en stopwords.\n",
    "                            if wo in Spanish_vocab:  #solo las que estan en el diccionarios Español\n",
    "                                #wo=wo.encode('utf-8') \n",
    "                                tokens.append(wo)                            \n",
    "                ficLeido1.close()\n",
    "            except UnicodeDecodeError:     \n",
    "                print \"error en\", ruta+fic\n",
    "                return [\"---\"]\n",
    "   \n",
    "    return tokens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de palabras total 27553\n",
      "numero de palabras sin repetir 2972\n"
     ]
    }
   ],
   "source": [
    "rutas=[(\"FicDatos/REG/\",\"REG*.txt\"),(\"FicDatos/NO_REG/\",\"NR*.txt\")]\n",
    "\n",
    "allwordsSIN=words_all_file_SinCaraSTOPDIC(rutas) #recupero todas las palabras de los ficheros.\n",
    "\n",
    "print \"Numero de palabras total\", len(allwordsSIN)  #Numero de palabras en total de todas las canciones.\n",
    "\n",
    "all_words = nltk.FreqDist(w.lower() for w in allwordsSIN) #frecuencia de cada palabra, de todos los documentos.\n",
    "word_features = list(all_words)[:3000]  #Me quedo con las 3000 palabras de la lista\n",
    "print \"numero de palabras sin repetir\", len(all_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "featuresets = [(document_features(d), c) for (d,c) in documents]\n",
    "train_set, test_set = featuresets[100:], featuresets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))  #Tiene un acierto superior al anterior.\n",
    "#classifier.show_most_informative_features(10) #las 10 palabras mas identificativas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caracteristicas mas Representativas: \n",
      "          contains(duro) = True              REG : NO_REG =     11.0 : 1.0\n",
      "          contains(vida) = True           NO_REG : REG    =      9.2 : 1.0\n",
      "          contains(alma) = True           NO_REG : REG    =      9.0 : 1.0\n",
      "          contains(ropa) = True              REG : NO_REG =      7.7 : 1.0\n",
      "      contains(haciendo) = True              REG : NO_REG =      7.7 : 1.0\n",
      "           contains(par) = True              REG : NO_REG =      7.7 : 1.0\n",
      "      contains(corazón) = True           NO_REG : REG    =      6.7 : 1.0\n",
      "           contains(fin) = True           NO_REG : REG    =      6.6 : 1.0\n",
      "        contains(hombre) = True              REG : NO_REG =      6.6 : 1.0\n",
      "         contains(llama) = True              REG : NO_REG =      6.6 : 1.0\n",
      "         contains(ritmo) = True              REG : NO_REG =      6.3 : 1.0\n",
      "            contains(pa) = True              REG : NO_REG =      6.3 : 1.0\n",
      "         contains(gusta) = True              REG : NO_REG =      6.1 : 1.0\n",
      "         contains(disco) = True              REG : NO_REG =      5.9 : 1.0\n",
      "     contains(demasiado) = True           NO_REG : REG    =      5.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cpdist = classifier._feature_probdist\n",
    "        \n",
    "print('Caracteristicas mas Representativas: ')\n",
    "\n",
    "for (fname, fval) in classifier.most_informative_features(15):\n",
    "    def labelprob(l):\n",
    "        return cpdist[l, fname].prob(fval)\n",
    "\n",
    "    labels = sorted([l for l in classifier._labels\n",
    "                     if fval in cpdist[l, fname].samples()],\n",
    "                    key=labelprob)\n",
    "    if len(labels) == 1:\n",
    "        continue\n",
    "    l0 = labels[0]\n",
    "    l1 = labels[-1]\n",
    "    if cpdist[l0, fname].prob(fval) == 0:\n",
    "        ratio = 'INF'\n",
    "    else:\n",
    "        ratio = '%8.1f' % (cpdist[l1, fname].prob(fval) /\n",
    "                           cpdist[l0, fname].prob(fval))\n",
    "    print(('%24s = %-14r %6s : %-6s = %s : 1.0' %\n",
    "           (fname, fval, (\"%s\" % l1)[:6], (\"%s\" % l0)[:6], ratio)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "## Probando el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Vamos a probar con una cancion de Alex Ubago.\n",
    "cancion=\"Si ayer tuviste un dia gris, tranquila, yo haré canciones para ver si así consigo acerte sonreir, si \\\n",
    "lo q quieres es huir, camina, yo haré canciones para ver, si así consigo fuerzas pa' vivir... No tengo mas motivos\\\n",
    "para darte que este miedo que me dá, el no volver a verte, nunca más... Creo ver la lluvia caer en mi ventana te veo \\\n",
    "pero no está lloviendo no es más que un reflejo de mi pensamiento, hoy te echo de menos... Yo sólo quiero hacerte \\\n",
    "saber amiga estes donde estes que si te falta el aliento yo te lo daré, y si te sientes sóla hablame, que te estaré \\\n",
    "escuchando aunque no te pueda ver... aunque no te pueda ver... De tantas cosas que perdí diría que sólo guardo lo que \\\n",
    "fue magico tiempo que nació en abril, miradas tristes sobre mi se anidan y se hacen parte de mi ser y ahora siempre \\\n",
    "llueve por que estoy sin ti... para darte que esta fría soledad, que necesito darte tantas cosas más... Creo ver la \\\n",
    "lluvia caer en mi ventana te veo pero no está lloviendo no es mas que un reflejo de mi pensamiento hoy te echo de \\\n",
    "menos... Yo sólo quiero hacer saber amiga estes donde estes que si te falta aliento yo te lo daré...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancionSubago=nltk.word_tokenize(cancion.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO_REG'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(document_features(cancionSubago))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "letra2=\"ella no ta' enamora' de mi Yo tampoco Pero le gusta como yo le doy Yo la pongo a volar Cuando yo \\\n",
    "le doy besos Pero no ta' enamora' de mi Ella solamente quiere pasar un momento lunático \\\n",
    "She said she's not in love with me, no! No! But she loves how I give it to her. Yes! Yes! Sexy! \\\n",
    "And I blow her mind! Every time that I kiss her But she said she's not in love with me. \\\n",
    "Yeah right! Ella siempre me llama a las 3AM Dice que soy su pana El que le quita las ganas \\\n",
    "Ha ha ha! Y si ella supiera que me llama su hermana Porque tambien soy su pana \\\n",
    "El que le quita las ganasWho the hell is this? Texting me at 3:46AM It must be a trick \\\n",
    "And if she texting me at this time She's looking for a Richard Or should I say Dick \\\n",
    "Mami yo te doy pao! pao! Ella dice que yo soy mal hablao \\\n",
    "No mamita yo no soy mal hablao Que yo te hablo directo de Miami al Cibao So deja el papelazo \\\n",
    "Para los turistas y los payasos Hablame claro que tu sabes bien Que te gusta lo malo Dale palo!  \\\n",
    "Yo te doy caramelo Pero deja los celos Ella dice que no esta enamorada de mi \\\n",
    "Eso me conviene Don Miguelo Ella no ta' enamora' de mi no! no! \\\n",
    "Pero le gusta como yo le doy. si! si! Yo la pongo a volar Cuando yo le doy besos \\\n",
    "Pero no ta' enamora' de mi no! no! She said she's not in love with me, no! No! \\\n",
    "But she loves how I give it to her. Yes! Yes! Sexy! And I blow her mind! \\\n",
    "Every time that I kiss her But she said she's not in love with me. Yeah right. \\\n",
    "Casi siempre me llama a las 3AM Dice que soy su pana El que le quita las ganas \\\n",
    "Yo nunca la dejo a medias Tu entiende el drama Dice que soy su pana \\\n",
    "El que le quita las ganas Hemos hecho de to' Muchas posiciones en la escalera \\\n",
    "Amarrao en la cama Yo encima de ella Amanecimos en la bañera \\\n",
    "Y me la comi embarra de nutella Son requisitos que otros no nan usado \\\n",
    "Y ella solo dice wow Lo que yo hago nunca se ha inventado \\\n",
    "Porque ni el kamasutra Se lo ha inventado Oiga! \\\n",
    "Ella no ta' enamora' de mi no! no! Pero le gusta como yo le doy. si! si! \\\n",
    "Yo la pongo a volar Cuando yo le doy besos Pero no ta' enamora' de mi no! no! \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REG'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancionR=nltk.word_tokenize(letra2.decode('utf-8'))\n",
    "classifier.classify(document_features(cancionR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REG'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.prob_classify(document_features(cancionR)).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
